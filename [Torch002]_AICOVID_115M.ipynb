{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Torch002] AICOVID_115M.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zyFHSqmnlexS",
        "9RPgo8DvmDeu",
        "gTp2e79LSHVx",
        "_GxCO0fvq1yr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcdabaa0e90e419cb0c52ded5ddcad8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_baa65562ee344a40929e766eec034a21",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b101793740f144c69e0731e2a2103e62",
              "IPY_MODEL_3ee53b55fb44438e8d291d53343d5eb9"
            ]
          }
        },
        "baa65562ee344a40929e766eec034a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b101793740f144c69e0731e2a2103e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1944cff02304951875f5f005baac129",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6394c2a17e9b4eb6a8846543e7b80f5b"
          }
        },
        "3ee53b55fb44438e8d291d53343d5eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aaa646c59c3141eb83c7923404ddf488",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:12&lt;00:00, 3.86MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea77d0a852b34033adefe81f5aa02ba7"
          }
        },
        "f1944cff02304951875f5f005baac129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6394c2a17e9b4eb6a8846543e7b80f5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aaa646c59c3141eb83c7923404ddf488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea77d0a852b34033adefe81f5aa02ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28b2be4ca26d48e388a595d14ca562a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0dc2564da29549eb95b072db5efddc9e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0b89e709fa8e42a09808e1a4bdc64ce0",
              "IPY_MODEL_43c52e5dcb97406587e314d66ae8bd5f"
            ]
          }
        },
        "0dc2564da29549eb95b072db5efddc9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "0b89e709fa8e42a09808e1a4bdc64ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5fbd2b9522b44bbc941d22d84a810401",
            "_dom_classes": [],
            "description": "Validation sanity check:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8386d32879834d24b00a3a6ba0dbc19d"
          }
        },
        "43c52e5dcb97406587e314d66ae8bd5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6228a4f7459f4a3582eec04bf6d6e240",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9736f7c7c8644a6db493d79c0a317c91"
          }
        },
        "5fbd2b9522b44bbc941d22d84a810401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8386d32879834d24b00a3a6ba0dbc19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6228a4f7459f4a3582eec04bf6d6e240": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9736f7c7c8644a6db493d79c0a317c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f6bcf4724244f8088a22c79281ade4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f41b2c4854d4ccbbc92f2863bbf6985",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3154237f06a04290b4f7379786faecfc",
              "IPY_MODEL_2a7515a63aee4e1d893be2892db47ca0"
            ]
          }
        },
        "7f41b2c4854d4ccbbc92f2863bbf6985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "3154237f06a04290b4f7379786faecfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a74d8df5e90447893e2aa7fb246d2f8",
            "_dom_classes": [],
            "description": "Epoch 0:   7%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 150,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9bb3f65463b74c1f9afd1a018fea6e14"
          }
        },
        "2a7515a63aee4e1d893be2892db47ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_38eb80b3b0dc4c38b21af93c7e16570f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/150 [00:15&lt;03:30,  1.50s/it, loss=1.61, v_num=exp1, lr=0.002, val_loss=0.679, val_acc=0.625, train_loss=2.400, train_acc=0.375]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99cbeb689c174972a670c51f78c8146a"
          }
        },
        "7a74d8df5e90447893e2aa7fb246d2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9bb3f65463b74c1f9afd1a018fea6e14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38eb80b3b0dc4c38b21af93c7e16570f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99cbeb689c174972a670c51f78c8146a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlozg/aicovid/blob/main/%5BTorch002%5D_AICOVID_115M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYEmHeR7z_l1"
      },
      "source": [
        "Trong thử nghiệm 001, mình thử chunking data và chạy ResNet18. Kết quả đạt được tập train và chunking validation khá ấn tượng (93%). Và khi test trên data không chunking thì kết quả được 94%. Tuy nhiên khi đem nộp tập test thì chỉ được 53%. Điều này đã cho thấy những vấn đề sau với mô hình:\n",
        "- Chunking chỉ là phương pháp shift audio --> Mình nghĩ nó không có ý nghĩa nhiều khi dùng CNN vì tính chất spacial invariant.\n",
        "- Dữ liệu chưa đủ lớn (7k chunks thì chưa ổn lắm đâu).\n",
        "- Không thể áp dụng được major voting trên dữ liệu test chunk (bị lỗi liên quan tới boolean indexing với cuda).\n",
        "- Validation dataloader là dữ liệu đã chunked --> Không phản ánh đúng thực tế khi sử dụng.\n",
        "\n",
        "Trong thử nghiệm này mình sẽ:\n",
        "- Cải thiện augmentation.\n",
        "- Tạo các dataloader tốt hơn.\n",
        "- Tổ chức lại notebook khoa học hơn.\n",
        "\n",
        "Các trực quan khám phá được để ở trong notebook [Explore002](https://colab.research.google.com/drive/1Y8fV_L70dBqh8gv5qv08pHXpAcjbTUSu). Notebook này chỉ chứa đoạn code dùng để huấn luyện."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsOJoIodiHt1",
        "cellView": "form"
      },
      "source": [
        "#@title Lấy xác thực google để upload/download file\n",
        "#@markdown Vui lòng bấm vào link khi được yêu cầu và lấy mã để nhập vào\n",
        "\n",
        "# Xác thực google để upload/download qua google drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth =  GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNJEzB9xBAIW",
        "cellView": "form"
      },
      "source": [
        "#@markdown Hàm quản lý upload file\n",
        "def driveUpload(file_path, parent_id, file_name=None):\n",
        "  if file_name == None:\n",
        "    file_name = file_path.split('/')[-1]\n",
        "  # Kiểm tra file tồn tại\n",
        "  file_list = drive.ListFile({'q': f\"'{parent_id}' in parents and title = '{file_name}'\"}).GetList()\n",
        "  if len(file_list) > 1:\n",
        "    for file1 in file_list:\n",
        "      print('title: %s, id: %s' % (file1['title'], file1['id']))\n",
        "    raise NameError('More than 1 file with same name exist, please resolve this')\n",
        "  \n",
        "  elif len(file_list) == 0:\n",
        "    # File chưa có thì tạo mới\n",
        "    file = drive.CreateFile({'title': file_name, \n",
        "                             'parents': [{'id': parent_id}]})\n",
        "\n",
        "  else:\n",
        "    # Tồn tại duy nhất 1 file\n",
        "    file = file_list[0]\n",
        "  \n",
        "  file.SetContentFile(file_path)\n",
        "  file.Upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tI7SwbHneJD",
        "cellView": "form"
      },
      "source": [
        "#@markdown Hàm quản lý download file theo tên\n",
        "def driveDownload(file_name, parent_id):\n",
        "  # Kiểm tra file tồn tại\n",
        "  file_list = drive.ListFile({'q': f\"'{parent_id}' in parents and title = '{file_name}'\"}).GetList()\n",
        "  if len(file_list) > 1:\n",
        "    for file1 in file_list:\n",
        "      print('title: %s, id: %s' % (file1['title'], file1['id']))\n",
        "    raise NameError('More than 1 file with same name exist, please resolve this')\n",
        "  \n",
        "  elif len(file_list) == 0:\n",
        "    raise NameError(f'File named {file_name} not exist')\n",
        "\n",
        "  else:\n",
        "    # Tồn tại duy nhất 1 file\n",
        "    file = file_list[0]\n",
        "  \n",
        "  file.GetContentFile(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_pyzy0LjMrg"
      },
      "source": [
        "# Detect COVID-19 patients via forced-cough cell phone recording\n",
        "\n",
        "- **Bài toán**: Nhận diện người nhiễm COVID-19 qua tiếng ho ép buộc\n",
        "    - **Input**: Đoạn ghi âm tiếng ho, tuổi và giới tính\n",
        "    - **Output**: Phân loại người nhiễm bệnh hay không"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd_YA074tx3l"
      },
      "source": [
        "## Tìm hiểu bài toán \n",
        "Qua paper (https://dspace.mit.edu/bitstream/handle/1721.1/128954/09208795.pdf?sequence=1&isAllowed=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pbmNRtmnFNl"
      },
      "source": [
        "# Các biến thiết lập cho thử nghiệm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9Pxvi0cclld"
      },
      "source": [
        "# Nếu muốn train mô hình thì set thành True\n",
        "train_mode = True\n",
        "experiment_id = '002'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPKjksovwzjL"
      },
      "source": [
        "# ID của folder lưu model trên drive\n",
        "model_zoo = 'secret'\n",
        "# ID của folder chứa submission\n",
        "submission_folder = 'secret'\n",
        "# Tên của file nén để nộp\n",
        "zip_name = f'Torch_ver{experiment_id}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPFXWIsEj6dO"
      },
      "source": [
        "val_split = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2RHG_z9nEsf"
      },
      "source": [
        "forced_sr = 8000 # -1 mean not enforced\n",
        "n_mfcc_ceptrum = 200\n",
        "n_delta_features = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6trLD9GlZeY"
      },
      "source": [
        "# Import và cài đặt thư viện"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ikasEdh-I_wD"
      },
      "source": [
        "# cài lib. note: cài xong phải restart runtime\n",
        "try:\n",
        "  import torchaudio\n",
        "  import pytorch_lightning\n",
        "except ImportError:\n",
        "  !pip install torchaudio\n",
        "  !pip install pytorch-lightning\n",
        "  exit() # Tự động tắt process (vì lightning cần TF mới hơn nên phải restart)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zceUdpcYV4aK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbbb588-f195-460a-c74c-ef08c7ead851"
      },
      "source": [
        "# Quản lý file, folder\n",
        "import os\n",
        "from shutil import copyfile, rmtree\n",
        "import gc \n",
        "\n",
        "# Xử lý audio\n",
        "import torchaudio\n",
        "\n",
        "# Hiện audio nghe thử\n",
        "import IPython.display as ipd\n",
        "from IPython.display import Audio, display\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from torch.utils import data as ptdata\n",
        "from torch.utils.data import Dataset, TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import torchmetrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyFHSqmnlexS"
      },
      "source": [
        "# Tải dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aZvrpobaYbVy",
        "cellView": "form"
      },
      "source": [
        "#@markdown Tải dữ liệu, bao gồm: warmup (public train, public test, private set)\n",
        "%%capture\n",
        "# download public train data\n",
        "# official link: https://drive.google.com/file/d/1MPhz3zYl2yefCq-J5XySbFJt99BfKIZD/view\n",
        "# personal link: https://drive.google.com/file/d/1hoGLxjLmPY-pX-jSVGIaWIZhovQBMKU1/view?usp=sharing\n",
        "if not os.path.isfile('./aicv115m_public_train.zip'):\n",
        "  !gdown --id 1MPhz3zYl2yefCq-J5XySbFJt99BfKIZD\n",
        "  !unzip -o aicv115m_public_train.zip\n",
        "\n",
        "# dowload public test data\n",
        "# official link: https://drive.google.com/file/d/1UrMudzopA3CyR1Ih2J63Kfi2mY_0uhRK/view\n",
        "# personal link: https://drive.google.com/file/d/1X7vOjHos9f9w48-iTWyu5JElFqCjcH_R/view?usp=sharing\n",
        "if not os.path.isfile('./aicv115m_public_test.zip'):\n",
        "  !gdown --id 1UrMudzopA3CyR1Ih2J63Kfi2mY_0uhRK\n",
        "  !unzip -o aicv115m_public_test.zip\n",
        "\n",
        "# dowload private test data\n",
        "# personal link: https://drive.google.com/file/d/1Ec64sSm2dZqe3da_LVyE_jUBD0DnLyqB/view?usp=sharing\n",
        "if not os.path.isfile('./aicv115m_private_test.zip'):\n",
        "  !gdown --id 1Ec64sSm2dZqe3da_LVyE_jUBD0DnLyqB\n",
        "  !unzip -o aicv115m_private_test.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RPgo8DvmDeu"
      },
      "source": [
        "# Setup thư mục chứa data và đọc meta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQu9TA9rtogL",
        "cellView": "form"
      },
      "source": [
        "#@markdown Giải nén data\n",
        "%%capture\n",
        "!unzip -n aicv115m_public_train/train_audio_files_8k.zip\n",
        "!unzip -n aicv115m_public_test/public_test_audio_files_8k.zip\n",
        "\n",
        "train_path = 'train_audio_files_8k/'\n",
        "test_path = 'public_test_audio_files_8k/'\n",
        "private_test_path = 'aicv115m_private_test/private_test_audio_files_8k/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDG2YKjYvtH-",
        "outputId": "e57b0f8b-4e16-453a-81e4-b31c240627ef"
      },
      "source": [
        "print(f'Train path: {train_path}')\n",
        "print(f'Test path: {test_path}')\n",
        "print(f'Private test path: {private_test_path}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train path: train_audio_files_8k/\n",
            "Test path: public_test_audio_files_8k/\n",
            "Private test path: aicv115m_private_test/private_test_audio_files_8k/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLyEqGLbunhR",
        "cellView": "form"
      },
      "source": [
        "#@markdown Đọc meta\n",
        "train_meta = pd.read_csv('aicv115m_public_train/metadata_train_challenge.csv')\n",
        "train_meta['file_path'] = train_path+train_meta['file_path']\n",
        "test_meta = pd.read_csv('aicv115m_public_test/metadata_public_test.csv')\n",
        "test_meta['file_path'] = test_path+test_meta['file_path']\n",
        "private_test_meta = pd.read_csv('aicv115m_private_test/metadata_private_test.csv')\n",
        "private_test_meta['file_path'] = private_test_path+private_test_meta['file_path']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlLbjcQdvZ7w",
        "outputId": "e28a97b2-4534-4e8c-f1a6-89121d951b4a"
      },
      "source": [
        "display(train_meta.shape)\n",
        "train_meta.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(1199, 5)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>subject_gender</th>\n",
              "      <th>subject_age</th>\n",
              "      <th>assessment_result</th>\n",
              "      <th>file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3284bcf1-2446-4f3a-ac66-14c76b294177</td>\n",
              "      <td>male</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0</td>\n",
              "      <td>train_audio_files_8k/3284bcf1-2446-4f3a-ac66-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>431334e1-5946-4576-bb51-8e342ccc22b4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>train_audio_files_8k/431334e1-5946-4576-bb51-8...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1d6fac4b-1e7f-4bdc-81cd-3a720bfbb1e1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>train_audio_files_8k/1d6fac4b-1e7f-4bdc-81cd-3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>c7ee0695-b2e7-4beb-b904-f1455c9609d9</td>\n",
              "      <td>male</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0</td>\n",
              "      <td>train_audio_files_8k/c7ee0695-b2e7-4beb-b904-f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dd541704-b696-4181-8fd8-816daac0fcf9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>train_audio_files_8k/dd541704-b696-4181-8fd8-8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   uuid  ...                                          file_path\n",
              "0  3284bcf1-2446-4f3a-ac66-14c76b294177  ...  train_audio_files_8k/3284bcf1-2446-4f3a-ac66-1...\n",
              "1  431334e1-5946-4576-bb51-8e342ccc22b4  ...  train_audio_files_8k/431334e1-5946-4576-bb51-8...\n",
              "2  1d6fac4b-1e7f-4bdc-81cd-3a720bfbb1e1  ...  train_audio_files_8k/1d6fac4b-1e7f-4bdc-81cd-3...\n",
              "3  c7ee0695-b2e7-4beb-b904-f1455c9609d9  ...  train_audio_files_8k/c7ee0695-b2e7-4beb-b904-f...\n",
              "4  dd541704-b696-4181-8fd8-816daac0fcf9  ...  train_audio_files_8k/dd541704-b696-4181-8fd8-8...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fxXmdwxvitS",
        "outputId": "28212e7f-84df-403a-da97-228c51ddfab1"
      },
      "source": [
        "display(test_meta.shape)\n",
        "test_meta.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(350, 4)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>subject_gender</th>\n",
              "      <th>subject_age</th>\n",
              "      <th>file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>66ef1f05-fbb0-44cb-8bdb-8eb4df83359a</td>\n",
              "      <td>female</td>\n",
              "      <td>28.0</td>\n",
              "      <td>public_test_audio_files_8k/66ef1f05-fbb0-44cb-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>73d13a12-f9bc-4554-af49-be24f6024a25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>public_test_audio_files_8k/73d13a12-f9bc-4554-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d27dbe98-e061-4018-9900-d1f1d47feab1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>public_test_audio_files_8k/d27dbe98-e061-4018-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>43c30e4c-5d35-4ebc-8235-8920b7688550</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>public_test_audio_files_8k/43c30e4c-5d35-4ebc-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1952aa84-d077-495d-a1a9-9686a30722e0</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>public_test_audio_files_8k/1952aa84-d077-495d-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   uuid  ...                                          file_path\n",
              "0  66ef1f05-fbb0-44cb-8bdb-8eb4df83359a  ...  public_test_audio_files_8k/66ef1f05-fbb0-44cb-...\n",
              "1  73d13a12-f9bc-4554-af49-be24f6024a25  ...  public_test_audio_files_8k/73d13a12-f9bc-4554-...\n",
              "2  d27dbe98-e061-4018-9900-d1f1d47feab1  ...  public_test_audio_files_8k/d27dbe98-e061-4018-...\n",
              "3  43c30e4c-5d35-4ebc-8235-8920b7688550  ...  public_test_audio_files_8k/43c30e4c-5d35-4ebc-...\n",
              "4  1952aa84-d077-495d-a1a9-9686a30722e0  ...  public_test_audio_files_8k/1952aa84-d077-495d-...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgMLDOItrKUM",
        "outputId": "e2e93db7-9919-43f5-9f83-83da225193cb"
      },
      "source": [
        "display(private_test_meta.shape)\n",
        "private_test_meta.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(450, 4)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>subject_gender</th>\n",
              "      <th>subject_age</th>\n",
              "      <th>file_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bce020a3-6ab7-46df-8a75-7f8009a1883e</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>aicv115m_private_test/private_test_audio_files...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>efe397fd-5ff1-41d8-b991-b8acdafd663c</td>\n",
              "      <td>male</td>\n",
              "      <td>45.0</td>\n",
              "      <td>aicv115m_private_test/private_test_audio_files...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5954077a-4c41-4a2e-9cad-e3bb2d6402c4</td>\n",
              "      <td>female</td>\n",
              "      <td>27.0</td>\n",
              "      <td>aicv115m_private_test/private_test_audio_files...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2b330c25-0816-480a-bb87-9d3d0d632c0c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>aicv115m_private_test/private_test_audio_files...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bfa78793-b3b8-42b8-bad0-77e3c55abfda</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>aicv115m_private_test/private_test_audio_files...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   uuid  ...                                          file_path\n",
              "0  bce020a3-6ab7-46df-8a75-7f8009a1883e  ...  aicv115m_private_test/private_test_audio_files...\n",
              "1  efe397fd-5ff1-41d8-b991-b8acdafd663c  ...  aicv115m_private_test/private_test_audio_files...\n",
              "2  5954077a-4c41-4a2e-9cad-e3bb2d6402c4  ...  aicv115m_private_test/private_test_audio_files...\n",
              "3  2b330c25-0816-480a-bb87-9d3d0d632c0c  ...  aicv115m_private_test/private_test_audio_files...\n",
              "4  bfa78793-b3b8-42b8-bad0-77e3c55abfda  ...  aicv115m_private_test/private_test_audio_files...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTp2e79LSHVx"
      },
      "source": [
        "# Hàm xử lý âm thanh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjCQwg-opB0L",
        "cellView": "form"
      },
      "source": [
        "#@markdown ## Các hàm vỏ bọc cho đọc file\n",
        "#@markdown `read_audio(path)`: vỏ bọc cho `torchaudio.load(path)`.<br>\n",
        "#@markdown `read_resample_audio(path)`: chỉ trả về wave vì sample rate đã được cố định.\n",
        "\n",
        "'''\n",
        "  Read audio from given path and return (wave, sample_rate)\n",
        "'''\n",
        "def read_audio(full_audio_path):\n",
        "  return torchaudio.load(full_audio_path)\n",
        "\n",
        "'''\n",
        "  Read audio from given path, then resample if sample rate is not matched \n",
        "  and return wave.\n",
        "\n",
        "  Tips: \n",
        "    you should provide resampler from torchaudio.transform\n",
        "    when batch resampling with same params since this can\n",
        "    give a huge speed up.\n",
        "'''\n",
        "def read_resample_audio(\n",
        "    full_audio_path, resample,\n",
        "    resampler=None\n",
        "):\n",
        "  wave, sr = torchaudio.load(full_audio_path)\n",
        "  if resampler is not None:\n",
        "      wave = resampler(wave)\n",
        "  elif sr != resample:\n",
        "      wave = torchaudio.functional.resample(wave, sr, resample)\n",
        "  return wave"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEboAF063rLi"
      },
      "source": [
        "## Audio features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPA3iLpQ1fhs"
      },
      "source": [
        "# Spectrogram transformation\n",
        "n_fft = 2048\n",
        "win_length = 160\n",
        "hop_length = 80\n",
        "n_mels = 200\n",
        "n_mfcc = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuHhNXmvCYuR",
        "cellView": "form"
      },
      "source": [
        "#@markdown `spectrogram(waveform)` --> spec \n",
        "spectrogram = torchaudio.transforms.Spectrogram(\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    center=True,\n",
        "    normalized=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        ")\n",
        "\n",
        "#@markdown `mel_spectrogram(waveform)` --> mel_spec \n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=8000,\n",
        "    n_fft=n_fft,\n",
        "    win_length=win_length,\n",
        "    hop_length=hop_length,\n",
        "    center=True,\n",
        "    pad_mode=\"reflect\",\n",
        "    power=2.0,\n",
        "    #norm='slaney',\n",
        "    onesided=True,\n",
        "    normalized=True,\n",
        "    n_mels=n_mels,\n",
        "    mel_scale=\"htk\",\n",
        ")\n",
        "\n",
        "#@markdown `log_spectrogram(spec)` --> log(spec)\n",
        "log_spectrogram = torchaudio.transforms.AmplitudeToDB(\n",
        "    stype='power',\n",
        "    top_db=80\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkxxsDa8r0ql"
      },
      "source": [
        "## Augmentation cho audio\n",
        "Bao gồm: thêm noise (nhiều mức độ), SpecAugment, chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp1GX9b5pN_C",
        "cellView": "form"
      },
      "source": [
        "#@markdown `AudioChunking(chunk_size=400, chunk_step=200)`\n",
        "class AudioChunking(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 chunk_size: int=400,\n",
        "                 chunk_step: int=200) -> None:\n",
        "        super(AudioChunking, self).__init__()\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_step = chunk_step\n",
        "        \n",
        "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
        "        _, _, spec_len = spec.shape\n",
        "        pad_size = self.chunk_size - spec_len%self.chunk_size\n",
        "        pad_size = (pad_size//2, pad_size//2+pad_size%2)\n",
        "        padded_spec = torch.nn.functional.pad(spec, pad_size, mode='constant', value=0)\n",
        "        chunks = padded_spec.unfold(-1, self.chunk_size, self.chunk_step).permute(2,0,1,3)\n",
        "        return chunks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "JK5OCYJfpYz6"
      },
      "source": [
        "#@markdown `SpecAugment(time_W=50, freq_W=50, T=80, F=80)`\n",
        "def _h_poly(t):\n",
        "    tt = t.unsqueeze(-2)**torch.arange(4, device=t.device).view(-1,1)\n",
        "    A = torch.tensor([\n",
        "        [1, 0, -3, 2],\n",
        "        [0, 1, -2, 1],\n",
        "        [0, 0, 3, -2],\n",
        "        [0, 0, -1, 1]\n",
        "    ], dtype=t.dtype, device=t.device)\n",
        "    return A @ tt\n",
        "\n",
        "\n",
        "def _cspline_interpolate(x, y, xs):\n",
        "    '''\n",
        "    Input x and y must be of shape (batch, n) or (n)\n",
        "    '''\n",
        "    m = (y[..., 1:] - y[..., :-1]) / (x[..., 1:] - x[..., :-1])\n",
        "    m = torch.cat([m[...,[0]], (m[...,1:] + m[...,:-1]) / 2, m[...,[-1]]], -1)\n",
        "    idxs = torch.searchsorted(x[..., 1:], xs)\n",
        "    dx = (x.take_along_dim(idxs+1, dim=-1) - x.take_along_dim(idxs, dim=-1))\n",
        "    hh = _h_poly((xs - x.take_along_dim(idxs, dim=-1)) / dx)\n",
        "    return hh[...,0,:] * y.take_along_dim(idxs, dim=-1) \\\n",
        "        + hh[...,1,:] * m.take_along_dim(idxs, dim=-1) * dx \\\n",
        "        + hh[...,2,:] * y.take_along_dim(idxs+1, dim=-1) \\\n",
        "        + hh[...,3,:] * m.take_along_dim(idxs+1, dim=-1) * dx\n",
        "        \n",
        "\n",
        "class SpecAugment(torch.nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      time_W: int = 50,\n",
        "      freq_W: int = 50,\n",
        "      T: int = 80,\n",
        "      F: int = 80,\n",
        "      mT: int = 1,\n",
        "      mF: int = 1\n",
        "  ) -> None:\n",
        "      super(SpecAugment, self).__init__()\n",
        "      self.time_W = time_W\n",
        "      self.freq_W = freq_W\n",
        "      if time_W==0 and freq_W==0:\n",
        "          self.cum_warping = lambda x: x\n",
        "      elif time_W!=0 and freq_W==0:\n",
        "          self.cum_warping = self.time_warping\n",
        "      elif time_W==0 and freq_W!=0:\n",
        "          self.cum_warping = self.freq_warping\n",
        "      else:\n",
        "          self.cum_warping = self.time_freq_warping\n",
        "      self.time_masking = torchaudio.transforms.TimeMasking(time_mask_param=T)\n",
        "      self.freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=F)\n",
        "\n",
        "\n",
        "  def _get_warping_flow(self,\n",
        "                        warp_p: torch.Tensor,\n",
        "                        warp_d: torch.Tensor,\n",
        "                        interp_len: int) -> torch.Tensor:\n",
        "      '''\n",
        "      Get interpolated flow\n",
        "      Warning: This function doesn't check for batch size match between warp_p and warp_d\n",
        "      '''\n",
        "      device = warp_p.device\n",
        "      batch_size = warp_p.shape[0]\n",
        "\n",
        "      src_control_points = torch.stack([torch.tensor([0], device=device).expand(batch_size),\n",
        "                                        warp_p, torch.tensor([interp_len-1], device=device).expand(batch_size)], dim=1)\n",
        "      dest_control_points = torch.stack([torch.tensor([-1.], device=device).expand(batch_size),\n",
        "                                        (warp_p-warp_d)*2/(interp_len-1)-1, torch.tensor([1], device=device).expand(batch_size)], dim=1)\n",
        "\n",
        "      # Interpolate from 3 points to interp_len points\n",
        "      src_interp_points = torch.linspace(0, interp_len-1, interp_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
        "      dest_interp_points = _cspline_interpolate(src_control_points, dest_control_points, src_interp_points)\n",
        "\n",
        "      return dest_interp_points\n",
        "\n",
        "\n",
        "  def freq_warping(self, specs: torch.Tensor) -> torch.Tensor:\n",
        "      '''\n",
        "      Frequency warping augmentation, only return interpolated flow\n",
        "\n",
        "      param:\n",
        "        specs: spectrogram of size (batch, channel, freq_bin, length)\n",
        "      '''\n",
        "      W = self.freq_W\n",
        "      device = specs.device\n",
        "      batch_size, _, num_freqs, num_frames = specs.shape\n",
        "\n",
        "      warp_p = torch.randint(W, num_freqs - W, (batch_size,), device=device)\n",
        "\n",
        "      # Uniform distribution from (0,W) with chance to be up to W negative\n",
        "      warp_d = torch.randint(-W, W, (batch_size,), device=device)\n",
        "      \n",
        "      dest_freq_points = self._get_warping_flow(warp_p, warp_d, num_freqs)\n",
        "      dest_frame_points = torch.linspace(-1, 1, num_frames, device=device)\n",
        "\n",
        "      grid = torch.cat(\n",
        "          (dest_frame_points.view(-1,1).expand(batch_size,num_freqs,-1,-1),\n",
        "          dest_freq_points.view(batch_size,-1,1,1).expand(-1,-1,num_frames,-1)), dim=-1)\n",
        "\n",
        "      return torch.nn.functional.grid_sample(specs, grid, align_corners=True)\n",
        "\n",
        "\n",
        "  def time_warping(self, specs: torch.Tensor) -> torch.Tensor:\n",
        "      '''\n",
        "      Time warping augmentation, only return interpolated flow\n",
        "\n",
        "      param:\n",
        "        specs: spectrogram of size (batch, channel, freq_bin, length)\n",
        "      '''\n",
        "      W = self.time_W\n",
        "      device = specs.device\n",
        "      batch_size, _, num_freqs, num_frames = specs.shape\n",
        "\n",
        "      warp_p = torch.randint(W, num_frames - W, (batch_size,), device=device)\n",
        "\n",
        "      # Uniform distribution from (0,W) with chance to be up to W negative\n",
        "      warp_d = torch.randint(-W, W, (batch_size,), device=device)\n",
        "\n",
        "      # Interpolate from 3 points to num_frames points\n",
        "      dest_frame_points = self._get_warping_flow(warp_p, warp_d, num_frames)\n",
        "      dest_freq_points = torch.linspace(-1, 1, num_freqs, device=device)\n",
        "\n",
        "      grid = torch.cat(\n",
        "          (dest_frame_points.view(batch_size,1,-1,1).expand(-1,num_freqs,-1,-1),\n",
        "          dest_freq_points.view(-1,1,1).expand(batch_size,-1,num_frames,-1)), dim=-1)\n",
        "\n",
        "      return torch.nn.functional.grid_sample(specs, grid, align_corners=True)\n",
        "\n",
        "\n",
        "  def time_freq_warping(self,specs: torch.Tensor) -> torch.Tensor:\n",
        "      '''\n",
        "      Doing both time warping and frequency warping augmentation\n",
        "\n",
        "      param:\n",
        "        specs: spectrogram of size (batch, channel, freq_bin, length)\n",
        "        W: strength of warp\n",
        "      '''\n",
        "      device = specs.device\n",
        "      batch_size, _, num_freqs, num_frames = specs.shape\n",
        "\n",
        "      time_warp_p = torch.randint(self.time_W, num_frames - self.time_W, (batch_size,), device=device)\n",
        "      freq_warp_p = torch.randint(self.freq_W, num_freqs - self.freq_W, (batch_size,), device=device)\n",
        "\n",
        "      # Uniform distribution from (0,W) with chance to be up to W negative\n",
        "      time_warp_d = torch.randint(-self.time_W, self.time_W, (batch_size,), device=device)\n",
        "      freq_warp_d = torch.randint(-self.freq_W, self.freq_W, (batch_size,), device=device)\n",
        "\n",
        "      # Interpolate lên theo kích thước spec\n",
        "      dest_freq_points = self._get_warping_flow(freq_warp_p, freq_warp_d, num_freqs)\n",
        "      dest_frame_points = self._get_warping_flow(time_warp_p, time_warp_d, num_frames)\n",
        "\n",
        "      grid = torch.cat(\n",
        "          (dest_frame_points.view(batch_size,1,-1,1).expand(-1,num_freqs,-1,-1),\n",
        "          dest_freq_points.view(batch_size,-1,1,1).expand(-1,-1,num_frames,-1)), dim=-1)\n",
        "\n",
        "      return torch.nn.functional.grid_sample(specs, grid, align_corners=True)\n",
        "\n",
        "\n",
        "  def forward(self, specs: torch.Tensor) -> torch.Tensor:\n",
        "      aug_specs = self.cum_warping(specs)\n",
        "      aug_specs = self.time_masking(aug_specs)\n",
        "      aug_specs = self.freq_masking(aug_specs)\n",
        "      return aug_specs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "fOlyVYDtuJDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f58e03-cbdd-41a0-904a-493143cf091b"
      },
      "source": [
        "#@markdown Tải noise audio\n",
        "import requests\n",
        "\n",
        "!mkdir _sample_data\n",
        "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
        "SAMPLE_NOISE_PATH = os.path.join('_sample_data', \"bg.wav\")\n",
        "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
        "SAMPLE_RIR_PATH = os.path.join('_sample_data', \"rir.wav\")\n",
        "\n",
        "def _fetch_data():\n",
        "  uri = [\n",
        "    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
        "    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH)\n",
        "  ]\n",
        "  for url, path in uri:\n",
        "    with open(path, 'wb') as file_:\n",
        "      file_.write(requests.get(url).content)\n",
        "\n",
        "_fetch_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘_sample_data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcKSZU1F4rAK"
      },
      "source": [
        "def _get_sample(path, resample=None):\n",
        "  effects = [\n",
        "    [\"remix\", \"1\"]\n",
        "  ]\n",
        "  if resample:\n",
        "    effects.extend([\n",
        "      [\"lowpass\", f\"{resample // 2}\"],\n",
        "      [\"rate\", f'{resample}'],\n",
        "    ])\n",
        "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
        "\n",
        "def get_noise_sample(*, resample=None):\n",
        "  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
        "\n",
        "def get_rir_sample(*, resample=None, processed=False):\n",
        "  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
        "  if not processed:\n",
        "    return rir_raw, sample_rate\n",
        "  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
        "  rir = rir / torch.norm(rir, p=2)\n",
        "  rir = torch.flip(rir, [1])\n",
        "  return rir, sample_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BEH8Qh2SqH6c"
      },
      "source": [
        "import math\n",
        "\n",
        "#@markdown `RoomReverb`, `NoiseInject`, `PhoneSim`\n",
        "class RoomReverb(torch.nn.Module):\n",
        "    def __init__(self, rir_list):\n",
        "        super(RoomReverb, self).__init__()\n",
        "        self.rirs = rir_list\n",
        "\n",
        "    def _get_rir(self):\n",
        "        if type(self.rirs) is list:\n",
        "            return random.choice(self.rirs)\n",
        "        else: \n",
        "            return next(self.rirs)\n",
        "\n",
        "    def forward(self, wave: torch.Tensor):\n",
        "        rir = self._get_rir()\n",
        "        _wave = torch.nn.functional.pad(wave, (rir.shape[-1]-1, 0))\n",
        "        _wave = torch.nn.functional.conv1d(_wave[None, ...], rir[None, ...])[0]\n",
        "        return _wave\n",
        "\n",
        "\n",
        "class NoiseInject(torch.nn.Module):\n",
        "    def __init__(self, noise_list, snr_db):\n",
        "        super(NoiseInject, self).__init__()\n",
        "        self.noises = noise_list\n",
        "        self.snr_db = snr_db\n",
        "\n",
        "    def _get_noise(self):\n",
        "        if type(self.noises) is list:\n",
        "            return random.choice(self.noises)\n",
        "        else: \n",
        "            return next(self.noises)\n",
        "\n",
        "    def forward(self, wave: torch.Tensor):\n",
        "        noise = self._get_noise()\n",
        "        _noise = noise.repeat(1, 1 + wave.shape[-1] // noise.shape[-1])[..., :wave.shape[-1]]\n",
        "        scale = math.exp(self.snr_db / 10) * _noise.norm(p=2) / wave.norm(p=2)\n",
        "        _wave = (scale * wave + _noise) / 2\n",
        "        return _wave\n",
        "\n",
        "\n",
        "class PhoneSim(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PhoneSim, self).__init__()\n",
        "\n",
        "    def forward(self, wave: torch.Tensor):\n",
        "        device = wave.device\n",
        "        _wave = wave.cpu()\n",
        "        _wave, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
        "          _wave, 8000,\n",
        "          effects=[[\"lowpass\", \"4000\"],\n",
        "                   [\"compand\", \"0.02,0.05\", \"-60,-60,-30,-10,-20,-8,-5,-8,-2,-8\", \"-8\", \"-7\", \"0.05\"]]\n",
        "        )\n",
        "        _wave = torchaudio.functional.apply_codec(_wave, 8000, format=\"gsm\")\n",
        "        return _wave.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6qT41qMqY10"
      },
      "source": [
        "rir, _ = get_rir_sample(resample=8000, processed=True)\n",
        "noise, _ = get_noise_sample(resample=8000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0yQgDHKF_VG",
        "cellView": "form"
      },
      "source": [
        "#@markdown `StandardScaler()`\n",
        "class StandardScaler(torch.nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(StandardScaler, self).__init__()\n",
        "        \n",
        "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
        "        return ((spec-spec.mean())/spec.std()).nan_to_num(posinf=0.0, neginf=0.0)\n",
        "\n",
        "#@markdown `MinMaxScaler()`\n",
        "class MinMaxScaler(torch.nn.Module):\n",
        "    def __init__(self, min=None, max=None) -> None:\n",
        "        super(MinMaxScaler, self).__init__()\n",
        "        if min:\n",
        "            self._min = lambda x: min\n",
        "        else:\n",
        "            self._min = lambda x: x.min()\n",
        "        if max:\n",
        "            self._max = lambda x: max\n",
        "        else:\n",
        "            self._max = lambda x: x.max()\n",
        "        \n",
        "    def forward(self, spec: torch.Tensor) -> torch.Tensor:\n",
        "        return ((spec-self._min(spec))/(self._max(spec)-self._min(spec))).nan_to_num(posinf=0.0, neginf=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Lc0HIA2ZjG"
      },
      "source": [
        "# Các hàm bổ trợ trực quan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQCI3iYC5fVe",
        "cellView": "form"
      },
      "source": [
        "#@markdown Vẽ specgram `plot_specgram(wave, sr, title, xlim, ylim)`\n",
        "#@markdown (specgram chỉ đơn giản là apply discrete-time Fourier transform)\n",
        "\n",
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  # Tensor --> Numpy\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  # Plot specgram for each channel\n",
        "  for c in range(num_channels):\n",
        "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Vẽ waveform `plot_waveform(wave, sr, title, xlim, ylim)`\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "  # Tensor --> Numpy\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "  # Plot waveform for each channel\n",
        "  for c in range(num_channels):\n",
        "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "    axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "      axes[c].set_ylim(ylim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Vẽ spectrogram `plot_spectrogram(spec, axs, title, ylabel, aspect, xmax)`\n",
        "\n",
        "def plot_spectrogram(spec, fig=None, axs=None, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "  if axs is None:\n",
        "    fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Spectrogram (db)')\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel('frame')\n",
        "  im = axs.imshow(log_spectrogram(spec), origin='lower', aspect=aspect)\n",
        "  if xmax:\n",
        "    axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Hiển thị audio box `play_audio(wave, sr)`\n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  if num_channels == 1:\n",
        "    display(Audio(waveform[0], rate=sample_rate))\n",
        "  elif num_channels == 2:\n",
        "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "  else:\n",
        "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1nyBy3_mgnh"
      },
      "source": [
        "# Đọc chuẩn bị dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlMNZ1_rqxex"
      },
      "source": [
        "## Tách validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyhxy5nzXdvN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a8f37107-9186-40cc-82af-f57656152550"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "idx_train, idx_val = train_test_split(train_meta.index,train_size=val_split)\n",
        "\n",
        "val_meta = train_meta.iloc[idx_val]\n",
        "train_meta = train_meta.iloc[idx_train]\n",
        "\n",
        "display(len(train_meta))\n",
        "display(len(val_meta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "959"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "240"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GxCO0fvq1yr"
      },
      "source": [
        "## Tạo dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkGwlGUh6iM0"
      },
      "source": [
        "class AICOVIDDataset(Dataset):\n",
        "    def __init__(self, meta_df, audio_transforms: torch.nn.ModuleList, stacking: bool=False):\n",
        "        self.meta_df = meta_df\n",
        "        self.transforms = audio_transforms\n",
        "        self.specs = []\n",
        "        self.idxs = []  # Cause 1 audio can be duplicated up to 4 by using 4 difference transformations\n",
        "                        # so we need index array use to query info from dataframe\n",
        "\n",
        "        for id, file in enumerate(self.meta_df['file_path']):\n",
        "            specs = self._read_spec_audio(file)\n",
        "            if stacking:\n",
        "                specs = torch.cat(specs)\n",
        "                self.specs += [specs]\n",
        "            else:\n",
        "                self.specs += specs\n",
        "            self.idxs += [id]*len(specs)\n",
        "\n",
        "        if stacking:\n",
        "            self.specs = torch.cat(self.specs)\n",
        "        \n",
        "    def _read_spec_audio(self, file):\n",
        "        wave = read_resample_audio(file, 8000).cuda()\n",
        "        specs = [trans(wave) for trans in self.transforms]\n",
        "        return specs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.specs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spec = self.specs[idx]\n",
        "        meta = self.meta_df.iloc[self.idxs[idx]]\n",
        "        try:\n",
        "            label = torch.tensor(meta['assessment_result'])\n",
        "        except KeyError:\n",
        "            label = None\n",
        "        id = meta['uuid']\n",
        "        gender = meta['subject_gender']\n",
        "        age = meta['subject_age']\n",
        "\n",
        "        return spec, label, id, gender, age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtkD8mqEuRPq"
      },
      "source": [
        "# Đọc, nhân bản và rút trích mel spectrogram\n",
        "basic_transform = torch.nn.Sequential(mel_spectrogram,\n",
        "                                      log_spectrogram,\n",
        "                                      StandardScaler()).cuda()\n",
        "transform0 = torch.nn.Sequential(basic_transform).cuda()\n",
        "transform1 = torch.nn.Sequential(NoiseInject([noise.cuda()], 8),\n",
        "                                 basic_transform).cuda()\n",
        "transform2 = torch.nn.Sequential(NoiseInject([noise.cuda()], 16),\n",
        "                                 basic_transform).cuda()\n",
        "transform3 = torch.nn.Sequential(RoomReverb([rir.cuda()]), \n",
        "                                 NoiseInject([noise.cuda()], 8), \n",
        "                                 PhoneSim(),\n",
        "                                 basic_transform).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ9YEQoR7TnU"
      },
      "source": [
        "chunking = AudioChunking(400, 200)\n",
        "transform0_chunking = torch.nn.Sequential(transform0,\n",
        "                                 chunking).cuda()\n",
        "transform1_chunking = torch.nn.Sequential(transform1,\n",
        "                                 chunking).cuda()\n",
        "transform2_chunking = torch.nn.Sequential(transform2,\n",
        "                                 chunking).cuda()\n",
        "transform3_chunking = torch.nn.Sequential(transform3,\n",
        "                                 chunking).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUs5ryz90RW-"
      },
      "source": [
        "train_set = AICOVIDDataset(train_meta, torch.nn.ModuleList([transform0_chunking,transform1_chunking,transform2_chunking,transform3_chunking]),\n",
        "                           stacking=True)\n",
        "val_set = AICOVIDDataset(val_meta, torch.nn.ModuleList([basic_transform]))\n",
        "test_set = AICOVIDDataset(test_meta, torch.nn.ModuleList([basic_transform]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsME_19Pw3P6"
      },
      "source": [
        "# TEST DATALOADER\n",
        "train_set[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbgKr7NB2GGf"
      },
      "source": [
        "len(train_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37j76Vm4q9P2"
      },
      "source": [
        "## Tạo dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ5DqBgfjsPB"
      },
      "source": [
        "# def overlap_chunking(tensor, chunk_size, chunk_step):\n",
        "#   # Chunk_num x MFCC_features x chunk_size\n",
        "#   chunks = tensor.unfold(1, chunk_size, chunk_step).permute(1,0,2)\n",
        "#   return chunks[ torch.amax(chunks, dim=[1,2]) > 1.7 ]\n",
        "\n",
        "def collate_chunking_fn(batch):\n",
        "    # A data tuple has the form:\n",
        "    # spec, label\n",
        "\n",
        "    specs, labels = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for spec, label, _, _, _ in batch:\n",
        "        chunks = chunking(spec)\n",
        "        specs += [chunks]\n",
        "        labels += [label]*chunks.size()[0]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    specs = torch.cat(specs)\n",
        "    try:\n",
        "      labels = torch.stack(labels)\n",
        "      return specs, labels\n",
        "    except:\n",
        "      return specs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpswQ8ayvZOo"
      },
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_pad_seq_fn(batch):\n",
        "    # A data tuple has the form:\n",
        "    # spec, label\n",
        "\n",
        "    specs, labels = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for spec, label, _, _, _ in batch:\n",
        "        specs += [spec.permute(2,0,1)]\n",
        "        labels += [label]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    specs = pad_sequence(specs, batch_first=True).permute(0,2,3,1)\n",
        "    try:\n",
        "      labels = torch.stack(labels)\n",
        "      return specs, labels\n",
        "    except:\n",
        "      return specs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpCUnnvX2Ajc"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    # A data tuple has the form:\n",
        "    # spec, label\n",
        "\n",
        "    specs, labels = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for spec, label, _, _, _ in batch:\n",
        "        specs += [spec]\n",
        "        labels += [label]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    specs = torch.stack(specs)\n",
        "    try:\n",
        "      labels = torch.stack(labels)\n",
        "      return specs, labels\n",
        "    except:\n",
        "      return specs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMXgqthQO-1W"
      },
      "source": [
        "chunking_batch_size = 32\n",
        "batch_size = 16\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device == \"cuda\":\n",
        "    num_workers = 2\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZb3WPwWPLF7"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=chunking_batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad_seq_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad_seq_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5-CcdYh8y5P"
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(\n",
        "    AICOVIDDataset(val_meta, torch.nn.ModuleList([basic_transform])),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad_seq_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82w-EE45TNFA"
      },
      "source": [
        "# Giám sát các biến chưa được xóa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdJx-Bj0JxQI"
      },
      "source": [
        "def pretty_size(size):\n",
        "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
        "\tassert(isinstance(size, torch.Size))\n",
        "\treturn \" × \".join(map(str, size))\n",
        "\n",
        "def dump_tensors(gpu_only=True):\n",
        "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
        "\timport gc\n",
        "\ttotal_size = 0\n",
        "\tfor obj in gc.get_objects():\n",
        "\t\ttry:\n",
        "\t\t\tif torch.is_tensor(obj):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj), \n",
        "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
        "\t\t\t\t\ttotal_size += obj.numel()\n",
        "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
        "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
        "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj), \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
        "\t\t\t\t\ttotal_size += obj.data.numel()\n",
        "\t\texcept Exception as e:\n",
        "\t\t\tpass        \n",
        "\tprint(\"Total size:\", total_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSF5aC4Ti4nT"
      },
      "source": [
        "# Huấn luyện mô hình (có thể bỏ qua vì mô hình đã save trên drive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y9lvxslO4sU"
      },
      "source": [
        "## Mô hình ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfp2ZvXqO7Nx"
      },
      "source": [
        "class AICOVIDModule(pl.LightningModule):\n",
        "    def __init__(self, model, learning_rate, augment=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        if augment:\n",
        "            self.augment = augment\n",
        "        else:\n",
        "            self.augment = lambda x: x\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lr = learning_rate\n",
        "        self.acc_metric = torchmetrics.Accuracy()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def cross_entropy_loss(self, logits, labels):\n",
        "        return F.nll_loss(logits, labels)\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x, y = train_batch\n",
        "        x = self.augment(x)\n",
        "        logits = self.forward(x)\n",
        "        \n",
        "        # negative log-likelihood for a tensor of size (batch x n_output)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        acc = self.acc_metric(logits, y)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_acc', acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x, y = val_batch\n",
        "        logits = self.forward(x)\n",
        "\n",
        "        # negative log-likelihood for a tensor of size (batch x n_output)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        acc = self.acc_metric(logits, y)\n",
        "        \n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "\n",
        "    def test_step(self, test_batch, batch_idx):\n",
        "        x, y = test_batch\n",
        "        logits = self.forward(x)\n",
        "\n",
        "        # negative log-likelihood for a tensor of size (batch x n_output)\n",
        "        loss = self.cross_entropy_loss(logits, y)\n",
        "        acc = self.acc_metric(logits, y)\n",
        "        \n",
        "        self.log('test_loss', loss, prog_bar=True)\n",
        "        self.log('test_acc', acc, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=(self.lr or self.learning_rate))\n",
        "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, max_lr=(self.lr or self.learning_rate), \n",
        "                                                             steps_per_epoch=len(self.train_dataloader()) // self.trainer.accumulate_grad_batches, epochs=self.trainer.max_epochs)\n",
        "        sched = {\n",
        "            'scheduler': self.scheduler,\n",
        "            'interval': 'step'\n",
        "        }\n",
        "        return [self.optimizer], [sched]\n",
        "\n",
        "    def on_epoch_start(self):\n",
        "        self.log('lr', self.scheduler.get_lr()[0], prog_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2_CcrZS5803"
      },
      "source": [
        "def reset_weight(model):\n",
        "  model.load_state_dict(torch.load('init_weights.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeL6iyCEryP5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "bcdabaa0e90e419cb0c52ded5ddcad8e",
            "baa65562ee344a40929e766eec034a21",
            "b101793740f144c69e0731e2a2103e62",
            "3ee53b55fb44438e8d291d53343d5eb9",
            "f1944cff02304951875f5f005baac129",
            "6394c2a17e9b4eb6a8846543e7b80f5b",
            "aaa646c59c3141eb83c7923404ddf488",
            "ea77d0a852b34033adefe81f5aa02ba7"
          ]
        },
        "outputId": "3525df0e-e404-4569-ba92-980ac86a3a3d"
      },
      "source": [
        "resnet = resnet18(pretrained=True)\n",
        "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "num_filters = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_filters, 2)\n",
        "model = AICOVIDModule(resnet, learning_rate=0.05)\n",
        "torch.save(model.state_dict(), 'init_weights.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcdabaa0e90e419cb0c52ded5ddcad8e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46830571.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkreOedHmIxq"
      },
      "source": [
        "## Profiling, kiểm tra bottleneck"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM26EGWkIMu8"
      },
      "source": [
        "# logger = pl.loggers.TensorBoardLogger(\n",
        "#                 save_dir='.',\n",
        "#                 version='benchmarking',\n",
        "#                 name='lightning_logs'\n",
        "#                 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW7WNiIGd3ZQ"
      },
      "source": [
        "# # Thử 5 epoch để benchmarking\n",
        "# reset_weight(model)\n",
        "# trainer = pl.Trainer(gpus=1,\n",
        "#                      profiler=\"simple\", logger=logger, log_gpu_memory='all', max_epochs=3, progress_bar_refresh_rate=1)\n",
        "# trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kOPNwZBmUX7"
      },
      "source": [
        "## Train hoàn chỉnh 60 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ4Pql7cFKmq"
      },
      "source": [
        "Tạo callback tự động backup model lightning logs lên drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuxxKOO_FCaj"
      },
      "source": [
        "from pytorch_lightning.callbacks import Callback\n",
        "\n",
        "class BackupCallback(Callback):\n",
        "    def on_train_end(self, trainer, pl_module):\n",
        "        if (trainer.current_epoch+1)%20 == 0:\n",
        "            os.system(\"zip ./tmp_lightning_logs.zip ./lightning_logs/*\")\n",
        "            try:\n",
        "                driveUpload(\"tmp_lightning_logs.zip\", model_zoo)\n",
        "            except:\n",
        "                print(\"Upload failed.\")\n",
        "            print(f\"Lightning logs backuped at epoch {trainer.current_epoch}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKJId8PN5qrW"
      },
      "source": [
        "## Thí nghiệm trên raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ES6rjb2O_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ded117-8702-41a7-b34b-122c6c66f7c5"
      },
      "source": [
        "try:\n",
        "    del train_set\n",
        "    del val_set\n",
        "    del train_loader\n",
        "    del val_loader\n",
        "except:\n",
        "    print(\"No variable to delete\")\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "pl.utilities.memory.garbage_collection_cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No variable to delete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9eG6pqCV_27",
        "outputId": "51455133-8b1d-424d-8865-d8dfe3bdfbab"
      },
      "source": [
        "(torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0))/1024"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2249.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbFiVzEVMh2J",
        "outputId": "4878f849-ed1f-4c52-cbfe-a2b928cf720f"
      },
      "source": [
        "print(torch.cuda.memory_summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    4467 MB |    8985 MB |  221297 MB |  216829 MB |\n",
            "|       from large pool |    4466 MB |    8984 MB |  192905 MB |  188439 MB |\n",
            "|       from small pool |       1 MB |     429 MB |   28391 MB |   28390 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    4467 MB |    8985 MB |  221297 MB |  216829 MB |\n",
            "|       from large pool |    4466 MB |    8984 MB |  192905 MB |  188439 MB |\n",
            "|       from small pool |       1 MB |     429 MB |   28391 MB |   28390 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    4470 MB |    9468 MB |    9468 MB |    4998 MB |\n",
            "|       from large pool |    4468 MB |    9020 MB |    9020 MB |    4552 MB |\n",
            "|       from small pool |       2 MB |     448 MB |     448 MB |     446 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2249 KB |  172691 KB |  234139 MB |  234137 MB |\n",
            "|       from large pool |    1482 KB |  171924 KB |  200692 MB |  200691 MB |\n",
            "|       from small pool |     767 KB |   69996 KB |   33447 MB |   33446 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |       7    |     974    |  161915    |  161908    |\n",
            "|       from large pool |       1    |     961    |   33531    |   33530    |\n",
            "|       from small pool |       6    |     806    |  128384    |  128378    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |       7    |     974    |  161915    |  161908    |\n",
            "|       from large pool |       1    |     961    |   33531    |   33530    |\n",
            "|       from small pool |       6    |     806    |  128384    |  128378    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       2    |     470    |     470    |     468    |\n",
            "|       from large pool |       1    |     246    |     246    |     245    |\n",
            "|       from small pool |       1    |     224    |     224    |     223    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       2    |     310    |   59609    |   59607    |\n",
            "|       from large pool |       1    |      14    |   15370    |   15369    |\n",
            "|       from small pool |       1    |     307    |   44239    |   44238    |\n",
            "|===========================================================================|\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnqtWtY72vBz"
      },
      "source": [
        "train_set = AICOVIDDataset(train_meta, torch.nn.ModuleList([basic_transform]))\n",
        "val_set = AICOVIDDataset(val_meta, torch.nn.ModuleList([basic_transform]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTy3Zayd3H3D"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_pad_seq_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad_seq_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6Dvej7UzDEn"
      },
      "source": [
        "### Thí nghiệm 1: train trên data raw (có pad sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t7-JkrFH2iL"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6ouAXvOHxbd"
      },
      "source": [
        "logger1 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp1',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660,
          "referenced_widgets": [
            "28b2be4ca26d48e388a595d14ca562a2",
            "0dc2564da29549eb95b072db5efddc9e",
            "0b89e709fa8e42a09808e1a4bdc64ce0",
            "43c52e5dcb97406587e314d66ae8bd5f",
            "5fbd2b9522b44bbc941d22d84a810401",
            "8386d32879834d24b00a3a6ba0dbc19d",
            "6228a4f7459f4a3582eec04bf6d6e240",
            "9736f7c7c8644a6db493d79c0a317c91",
            "3f6bcf4724244f8088a22c79281ade4b",
            "7f41b2c4854d4ccbbc92f2863bbf6985",
            "3154237f06a04290b4f7379786faecfc",
            "2a7515a63aee4e1d893be2892db47ca0",
            "7a74d8df5e90447893e2aa7fb246d2f8",
            "9bb3f65463b74c1f9afd1a018fea6e14",
            "38eb80b3b0dc4c38b21af93c7e16570f",
            "99cbeb689c174972a670c51f78c8146a"
          ]
        },
        "id": "Vho6NDEISt4F",
        "outputId": "0c90e46f-e508-43e7-e8ba-02b30781567f"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger1, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name       | Type     | Params\n",
            "----------------------------------------\n",
            "0 | model      | ResNet   | 11.2 M\n",
            "1 | acc_metric | Accuracy | 0     \n",
            "----------------------------------------\n",
            "11.2 M    Trainable params\n",
            "0         Non-trainable params\n",
            "11.2 M    Total params\n",
            "44.685    Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28b2be4ca26d48e388a595d14ca562a2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:1290: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f6bcf4724244f8088a22c79281ade4b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-b80ced45b0a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n\u001b[1;32m      3\u001b[0m                      logger=logger1, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    458\u001b[0m         )\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;31m# ------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m                 \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;31m# when returning -1 from train_step, we end epoch early\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                         \u001b[0;31m# optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                             \u001b[0;31m# revert back to previous state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mon_tpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDeviceType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_TPU_AVAILABLE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0musing_native_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0musing_native_amp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0musing_lbfgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_lbfgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         )\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m         \"\"\"\n\u001b[0;32m-> 1403\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_closure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure, *args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mprofiler_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"optimizer_step_and_closure_{self._optimizer_idx}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofiler_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_optimizer_step_calls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/optimizer.py\u001b[0m in \u001b[0;36m__optimizer_step\u001b[0;34m(self, closure, profiler_name, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofiler_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, opt_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m         )\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmake_optimizer_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mrun_optimizer_step\u001b[0;34m(self, optimizer, optimizer_idx, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     ) -> None:\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_zero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, lambda_closure, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_closure\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_closure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain_step_and_backward_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    731\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step_and_backward_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                             result = self.training_step_and_backward(\n\u001b[0;32m--> 733\u001b[0;31m                                 \u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m                             )\n\u001b[1;32m    735\u001b[0m                             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_step_and_backward\u001b[0;34m(self, split_batch, batch_idx, opt_idx, optimizer, hiddens)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step_and_backward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;31m# lightning module hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_curr_step_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, split_batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m                 \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-e22d8b9de7fe>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, train_batch, batch_idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# negative log-likelihood for a tensor of size (batch x n_output)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-e22d8b9de7fe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 218.00 MiB (GPU 0; 11.17 GiB total capacity; 10.28 GiB already allocated; 141.81 MiB free; 10.56 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2JKed1K8r13"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF8TdjmBztD5"
      },
      "source": [
        "### Thí nghiệm 2: thử data raw có SpecAugment, không chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NycvUrqF5mDR"
      },
      "source": [
        "model.augment = SpecAugment(time_W=100, freq_W=50, F=50, T=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWkfyfXH5mDR"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTIN5er05mDR"
      },
      "source": [
        "logger2 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp2',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlpSqUZi5mDS"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger2, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqTLJN298-4Z"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1atypbLj8Gcz"
      },
      "source": [
        "## Thí nghiệm trên raw data có chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDr89CXj8OpJ"
      },
      "source": [
        "try:\n",
        "    del train_set\n",
        "    del val_set\n",
        "    del train_loader\n",
        "    del val_loader\n",
        "except:\n",
        "    print(\"No variable to delete\")\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "pl.utilities.memory.garbage_collection_cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGHGU5Zg8OpK"
      },
      "source": [
        "train_set = AICOVIDDataset(train_meta, torch.nn.ModuleList([transform0_chunking]), stacking=True)\n",
        "val_set = AICOVIDDataset(val_meta, torch.nn.ModuleList([transform0_chunking]), stacking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHRQQjN8OpK"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgpDblpQzbhY"
      },
      "source": [
        "### Thí nghiệm 3: thử data raw có chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AGMUZ3t9Y9-"
      },
      "source": [
        "model = AICOVIDModule(resnet, learning_rate=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDkWzWWM9Y-G"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpelk3A49Y-H"
      },
      "source": [
        "logger3 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp3',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8F9VH1t9Y-H"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger3, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT-jx6Fo9Y-I"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijjGSjnx0dgf"
      },
      "source": [
        "### Thí nghiệm 4: thử data raw có chunking và SpecAugment từng chunk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjH7yV3A9f8A"
      },
      "source": [
        "model.augment = SpecAugment(time_W=100, freq_W=50, F=50, T=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEsrEZ1C9f8A"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xged8wH39f8A"
      },
      "source": [
        "logger4 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp4',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cI4Z2tf9f8B"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger4, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDRZgo2r9f8B"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWV8cMCu-E8S"
      },
      "source": [
        "## Thí nghiệm trên data noise inject không chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRWiwWhf-als"
      },
      "source": [
        "try:\n",
        "    del train_set\n",
        "    del val_set\n",
        "    del train_loader\n",
        "    del val_loader\n",
        "except:\n",
        "    print(\"No variable to delete\")\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "pl.utilities.memory.garbage_collection_cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HT59gAL-al3"
      },
      "source": [
        "train_set = AICOVIDDataset(train_meta, torch.nn.ModuleList([transform0,transform1,transform2,transform3]))\n",
        "val_set = AICOVIDDataset(val_meta, torch.nn.ModuleList([transform0,transform1,transform2,transform3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xAucJhz-al3"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_pad_seq_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_pad_seq_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OITNiN-ZzfcK"
      },
      "source": [
        "### Thí nghiệm 5: thử data noise injection, không chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uGjrVQ_9o8-"
      },
      "source": [
        "model = AICOVIDModule(resnet, learning_rate=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcv_xdf99o9A"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgeZaiBz9o9B"
      },
      "source": [
        "logger5 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp5',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it9_-pP_9o9B"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger5, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v33mzUdC9o9C"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_kPjpBiz432"
      },
      "source": [
        "### Thí nghiệm 6: thử data noise injection có SpecAugment, không chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyNwpFjb9p82"
      },
      "source": [
        "model.augment = SpecAugment(time_W=100, freq_W=50, F=50, T=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmplGIcy9p82"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPYsnsD19p82"
      },
      "source": [
        "logger6 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp6',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G70KHqql9p82"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger6, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgmtiepJ9p83"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-mlQzNq-MSr"
      },
      "source": [
        "## Thí nghiệm trên data noise inject có chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW9ltcdX_DdX"
      },
      "source": [
        "try:\n",
        "    del train_set\n",
        "    del val_set\n",
        "    del train_loader\n",
        "    del val_loader\n",
        "except:\n",
        "    print(\"No variable to delete\")\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "pl.utilities.memory.garbage_collection_cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYy36eBT_Ddf"
      },
      "source": [
        "train_set = AICOVIDDataset(train_meta, torch.nn.ModuleList([transform0_chunking,transform1_chunking,transform2_chunking,transform3_chunking]), stacking=True)\n",
        "val_set = AICOVIDDataset(val_meta, torch.nn.ModuleList([transform0_chunking,transform1_chunking,transform2_chunking,transform3_chunking]), stacking=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kX4kEkk_Ddg"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOuEl1x1zrDt"
      },
      "source": [
        "### Thí nghiệm 7: thử data noise injection, có chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv3Z2yfK9qzv"
      },
      "source": [
        "model = AICOVIDModule(resnet, learning_rate=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szLjbUcP9qzw"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKbn7qax9qzw"
      },
      "source": [
        "logger7 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp7',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqXl-atY9qzw"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger7, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8POBnx7r9qzw"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk1wQc0R7BZd"
      },
      "source": [
        "### Thí nghiệm 8: thử data noise injection có SpecAugment, có chunking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKVLinD99rXj"
      },
      "source": [
        "model.augment = SpecAugment(time_W=100, freq_W=50, F=50, T=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqcCvA4n9rXj"
      },
      "source": [
        "Tạo logger có tên cố định"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RmDqQIK9rXj"
      },
      "source": [
        "logger8 = pl.loggers.TensorBoardLogger(\n",
        "                save_dir='.',\n",
        "                version='train_60_epochs_exp8',\n",
        "                name='lightning_logs'\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxUP3Sef9rXj"
      },
      "source": [
        "reset_weight(model)\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=60, accumulate_grad_batches=2,\n",
        "                     logger=logger8, progress_bar_refresh_rate=10, callbacks=[BackupCallback()])\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmUEQKXM9rXj"
      },
      "source": [
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NdDA6uC13XV"
      },
      "source": [
        "# Đánh giá mô hình"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlzwfzAEwNxF"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqW-FwVCv5-m"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./lightning_logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQtRaoeOwVDP"
      },
      "source": [
        "## Xem thử các mẫu phân lớp sai"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwdDD6rw3OYP"
      },
      "source": [
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "\n",
        "def test(model):\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        #data = data.to(device)\n",
        "        #target = target.to(device)\n",
        "\n",
        "        output = trainer.call_hook('forward', data)\n",
        "\n",
        "        pred = get_likely_index(output)\n",
        "        correct += number_of_correct(pred, target)\n",
        "\n",
        "    print(f\"Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n\")\n",
        "\n",
        "test(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcFmptrlYET4"
      },
      "source": [
        "correct = 0\n",
        "preds = []\n",
        "for i in range(len(train_meta)):\n",
        "  x = read_MFCC_audio(train_meta['file_path'].iloc[i]).view(1,1,200,-1).cuda()\n",
        "  y = torch.Tensor([train_meta['assessment_result'].iloc[i]]).long().cuda()\n",
        "  output = trainer.call_hook('forward', x)\n",
        "\n",
        "  pred = get_likely_index(output)\n",
        "  preds += [pred.item()]\n",
        "  correct += number_of_correct(pred, y)\n",
        "\n",
        "correct/len(train_meta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvSlT6HzcTJn"
      },
      "source": [
        "confusion_matrix(train_meta['assessment_result'], preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zrmmQoum2wv"
      },
      "source": [
        "torch.Tensor([False, True, True]).nonzero()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FOvVAP2bRyL"
      },
      "source": [
        "correct = 0\n",
        "preds = []\n",
        "for i in range(len(train_meta)):\n",
        "  x = read_MFCC_audio(train_meta['file_path'].iloc[i]).cuda()\n",
        "  x = overlap_chunking(pad_tensor(x, 300), 300, 50, max_thresh=False).unsqueeze(1)\n",
        "  y = torch.Tensor([train_meta['assessment_result'].iloc[i]]).long().cuda()\n",
        "  output = trainer.call_hook('forward', x)\n",
        "\n",
        "  pred = get_likely_index(output)\n",
        "  pred = torch.mode(pred, 0)[0]\n",
        "  preds += [pred.item()]\n",
        "  correct += number_of_correct(pred, y)\n",
        "\n",
        "correct/len(train_meta)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLy5oVzDa_sC"
      },
      "source": [
        "confusion_matrix(train_meta['assessment_result'], preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdXrW8PC1qUN"
      },
      "source": [
        "# Dự đoán trên test set để submit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXnRu87vi4II"
      },
      "source": [
        "preds = []\n",
        "for i in range(len(private_test_meta)):\n",
        "  x = read_MFCC_audio(private_test_meta['file_path'].iloc[i]).view(1,1,200,-1).cuda()\n",
        "  output = trainer.call_hook('forward', x)\n",
        "\n",
        "  pred = get_likely_index(output)\n",
        "  preds += [pred.item()]\n",
        "np.array(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mlQ9N2J1x21"
      },
      "source": [
        "# Lưu kết quả"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb7KoY9Mi4Oj",
        "cellView": "form"
      },
      "source": [
        "#@markdown Lưu lại model lên Google Drive\n",
        "if train_mode:\n",
        "  os.system('mkdir trained_models')\n",
        "  compressed_name = f'{zip_name}_model.zip'\n",
        "  torch.save(model.state_dict(), './trained_models/model_weights.pth')\n",
        "  \n",
        "  os.system(f'zip -j ./{compressed_name} ./trained_models/*')\n",
        "  driveUpload(compressed_name, model_zoo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQMWT6nXWebS",
        "cellView": "form"
      },
      "source": [
        "#@markdown Lưu lại public test submission lên Google Drive\n",
        "submit_df = pd.DataFrame({'uuid': test_meta['uuid'],\n",
        "                          'assessment_result': preds})\n",
        "submit_df.to_csv('results.csv', index=False)\n",
        "\n",
        "# Nén file\n",
        "os.system(f'zip -j ./{zip_name}.zip ./results.csv')\n",
        "\n",
        "driveUpload(zip_name+'.zip', submission_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Bv4lQXEsrep-"
      },
      "source": [
        "#@markdown Lưu lại private test submission lên Google Drive\n",
        "submit_df = pd.DataFrame({'uuid': private_test_meta['uuid'],\n",
        "                          'assessment_result': preds})\n",
        "submit_df.to_csv('results.csv', index=False)\n",
        "\n",
        "# Nén file\n",
        "os.system(f'zip -j ./{zip_name}_private_test.zip ./results.csv')\n",
        "\n",
        "driveUpload(zip_name+'_private_test.zip', submission_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHzHpd7InP8M",
        "cellView": "form"
      },
      "source": [
        "#@markdown Load model lưu sẵn\n",
        "if not train_mode:\n",
        "  driveDownload(f'{zip_name}_model.zip', model_zoo)\n",
        "  os.system(f'unzip -o {zip_name}_model.zip')\n",
        "  model.load_state_dict(torch.load('model_weights.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}